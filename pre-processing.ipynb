{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root  = Path('../../Input/DAIC-WOZ')\n",
    "workspace  = Path('../../Working/DAIC-WOZ')\n",
    "\n",
    "audio_root = workspace/'audio'\n",
    "csv_root = workspace/'csv'\n",
    "\n",
    "train_audio_root = audio_root/'train'\n",
    "dev_audio_root = audio_root/'dev'\n",
    "test_audio_root = audio_root/'test'\n",
    "\n",
    "train_csv  = data_root/'train_split_Depression_AVEC2017.csv'\n",
    "dev_csv    = data_root/'dev_split_Depression_AVEC2017.csv'\n",
    "test_csv   = data_root/'test_split_Depression_AVEC2017.csv'\n",
    "\n",
    "audio_roots = {\n",
    "    'train': train_audio_root,\n",
    "    'dev'  : dev_audio_root,\n",
    "    'test' : test_audio_root,\n",
    "}\n",
    "\n",
    "csvs = {\n",
    "    'train': train_csv,\n",
    "    'dev'  : dev_csv,\n",
    "    'test' : test_csv,\n",
    "}\n",
    "\n",
    "id_cols = {\n",
    "    'train': 'Participant_ID',\n",
    "    'dev'  : 'Participant_ID',\n",
    "    'test' : 'participant_ID',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, csv in csvs.items():\n",
    "\n",
    "    audio_root = audio_roots[dataset]\n",
    "\n",
    "    df_dataset = pd.read_csv(csv)\n",
    "\n",
    "    ! mkdir -p {audio_root}\n",
    "\n",
    "    list_out = []\n",
    "\n",
    "    for _, sr in tqdm(df_dataset.iterrows(), total=len(df_dataset)):\n",
    "\n",
    "        id = sr[id_cols[dataset]].astype(int)\n",
    "\n",
    "        audio_dir = audio_root/f'{id}'\n",
    "        segment_dir = audio_dir/'segment'\n",
    "        zip_file = data_root/f'{id}_P.zip'\n",
    "        vision_feature_files = audio_dir/'*_CLNF_*'\n",
    "\n",
    "        ! mkdir -p {audio_dir}\n",
    "        ! mkdir -p {segment_dir}\n",
    "        ! unzip -q {zip_file} -d {audio_dir}\n",
    "        ! rm {vision_feature_files}\n",
    "\n",
    "        df = pd.read_csv(audio_dir/f'{id}_TRANSCRIPT.csv', sep='\\t')\n",
    "        df[['start_time_ms', 'stop_time_ms']] = df[['start_time', 'stop_time']].applymap(lambda x: 1000 * x).astype(int)\n",
    "        df = df[df['speaker'] == 'Participant'].reset_index(drop=True)\n",
    "\n",
    "        audio = AudioSegment.from_file(audio_dir/f'{id}_AUDIO.wav')\n",
    "\n",
    "        for i, sr_trans in df[['start_time_ms', 'stop_time_ms', 'value']].iterrows():\n",
    "\n",
    "            start, stop, script = sr_trans.to_list()\n",
    "            audio_file = (segment_dir/f'{i}.wav').resolve()\n",
    "\n",
    "            segment = audio[start: stop]\n",
    "            segment.export(audio_file, format='wav')\n",
    "            \n",
    "            additional_dict = {\n",
    "                'response_id': i + 1,\n",
    "                'audio_file': audio_file,\n",
    "                'script': script,\n",
    "            }\n",
    "            list_out.append({**(sr.to_dict()), **additional_dict})\n",
    "    \n",
    "    df_out = pd.DataFrame(list_out)\n",
    "    df_out.to_csv(f'{csv_root}/{dataset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
