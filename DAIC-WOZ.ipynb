{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc728931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q -U transformers==4.28.0 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eef57ba-2434-4539-8457-be04dd188f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoModelForAudioClassification, TrainingArguments, Trainer, pipeline\n",
    "from datasets import Dataset, Features, Value, Audio, ClassLabel, DatasetInfo, NamedSplit, DatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee609803-3b1d-4c57-993d-2cd09d214ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workspace  = Path('../../Working/DAIC-WOZ').resolve()\n",
    "\n",
    "csv_root = workspace/'csv'\n",
    "\n",
    "csvs = {\n",
    "    'train': csv_root/'train.csv',\n",
    "    'dev'  : csv_root/'dev.csv',\n",
    "    'test' : csv_root/'test.csv',\n",
    "}\n",
    "\n",
    "base_models = [\n",
    "    'facebook/wav2vec2-base',\n",
    "    'facebook/hubert-base-ls960',\n",
    "    'microsoft/wavlm-base-plus',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab66ec8a-2ac8-459d-b2f8-2fa4c63add44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv, istest=False):\n",
    "\n",
    "    old_title = [\n",
    "        'audio_file',\n",
    "        'audio_file',\n",
    "        'script',\n",
    "        'Participant_ID',\n",
    "        'response_id',\n",
    "        'audio_file',\n",
    "        'PHQ8_Binary',\n",
    "    ]\n",
    "\n",
    "    new_title = [\n",
    "        'file',\n",
    "        'audio',\n",
    "        'text',\n",
    "        'speaker_id',\n",
    "        'chapter_id',\n",
    "        'id',\n",
    "        'label',\n",
    "    ]\n",
    "\n",
    "    if istest:\n",
    "        old_title, new_title = old_title[:-1], new_title[:-1]\n",
    "        old_title[3] = 'participant_ID'\n",
    "\n",
    "    df = pd.read_csv(csv)[old_title]\n",
    "    df.columns=new_title\n",
    "    df['id'] = df['id'].map(lambda x: Path(x).stem)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7113145-bd79-434b-bf93-8045d571ec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_features_and_info(istest=False):\n",
    "\n",
    "    features_dict = {\n",
    "        'file': Value(dtype='string'),\n",
    "        'audio': Value(dtype='string'),\n",
    "        # 'audio': Audio(sampling_rate=16000, mono=True, decode=True),\n",
    "        'text': Value(dtype='string'),\n",
    "        'speaker_id': Value(dtype='int64'),\n",
    "        'chapter_id': Value(dtype='int64'),\n",
    "        'id': Value(dtype='string'),\n",
    "    }\n",
    "\n",
    "    if not istest:\n",
    "        features_dict['label'] = ClassLabel(num_classes=2, names=['healthy', 'depressed'])\n",
    "        \n",
    "    features = Features(features_dict)\n",
    "\n",
    "    info = DatasetInfo(\n",
    "        description='DAIC-WOZ',\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'features': features,\n",
    "        'info': info,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d37aae-2337-4f69-a80f-1672037321df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e9033245274f3a881d03381bd81720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    ds: Dataset.from_pandas(\n",
    "        load_csv(csv, istest=ds=='test'),\n",
    "        **make_features_and_info(istest=ds=='test'),\n",
    "        split=NamedSplit(ds),\n",
    "    ) for ds, csv in tqdm(csvs.items(), total=len(csvs))\n",
    "}) \\\n",
    ".cast_column('audio', Audio(\n",
    "    sampling_rate=16000,\n",
    "    mono=True,\n",
    "    decode=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd474b95-fdd0-477f-8064-02a7e77c495c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'label'],\n",
       "        num_rows: 16906\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'label'],\n",
       "        num_rows: 6679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 8816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47793b4-c1a7-4dc8-98c6-edfd4c7d7c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'label'],\n",
       "    num_rows: 16906\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f002920d-84e6-4c64-8e07-74fbaeea4984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': Value(dtype='string', id=None),\n",
       " 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'speaker_id': Value(dtype='int64', id=None),\n",
       " 'chapter_id': Value(dtype='int64', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['healthy', 'depressed'], id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53108a84-4f9c-44ac-8849-5dbb30aff0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/com.docker.devenvironments.code/Working/DAIC-WOZ/audio/train/303/segment/0.wav',\n",
       " 'audio': {'path': '/com.docker.devenvironments.code/Working/DAIC-WOZ/audio/train/303/segment/0.wav',\n",
       "  'array': array([ 0.01864624,  0.02023315,  0.02032471, ..., -0.00506592,\n",
       "         -0.00537109, -0.00537109], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': \"okay how 'bout yourself\",\n",
       " 'speaker_id': 303,\n",
       " 'chapter_id': 1,\n",
       " 'id': '0',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a32350e0-d9b1-4bc8-8430-91c31912e0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759732f50b9046148e6189d80b03e60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea376c81e7f84300bd2df4f7536e186f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.weight_proj.weight', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0345eacb33e9430d93242ea675f7cdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753fa328bce44242bd5481eb0a131372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22db8ce9f27d436fbc8e3c2eba4a1759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5990941f7f944c1a9d68c70ea353bb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(base_models[0])\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_models[0])\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(base_models[0])\n",
    "pipe = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc20ef4-e992-49ad-9eeb-7e9b2ab2c811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column('audio', Audio(sampling_rate=pipe.feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373543cb-f14a-47b7-98f3-e6afa05570d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc11cb4-5223-461b-b28c-0a5ecd0cfdef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_array_from_audio(dataset):\n",
    "    audio_arrays = [x[\"array\"] for x in dataset[\"audio\"]]\n",
    "    return audio_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0469dedd-7b27-4b83-a4d5-5564ec1a6d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.5021592378616333, 'label': 'LABEL_0'},\n",
       "  {'score': 0.4978407323360443, 'label': 'LABEL_1'}],\n",
       " [{'score': 0.5028910040855408, 'label': 'LABEL_1'},\n",
       "  {'score': 0.497109055519104, 'label': 'LABEL_0'}],\n",
       " [{'score': 0.5126659870147705, 'label': 'LABEL_0'},\n",
       "  {'score': 0.4873340129852295, 'label': 'LABEL_1'}],\n",
       " [{'score': 0.5033653378486633, 'label': 'LABEL_1'},\n",
       "  {'score': 0.4966345727443695, 'label': 'LABEL_0'}],\n",
       " [{'score': 0.5009957551956177, 'label': 'LABEL_1'},\n",
       "  {'score': 0.4990042448043823, 'label': 'LABEL_0'}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(get_array_from_audio(dataset['dev'][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10fe1f9-ac73-44c2-a763-c5dfed38e433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mAutoModelForAudioClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.\n",
      "\n",
      "The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
      "passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      "falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      "\n",
      "    - **audio-spectrogram-transformer** -- [`ASTForAudioClassification`] (Audio Spectrogram Transformer model)\n",
      "    - **data2vec-audio** -- [`Data2VecAudioForSequenceClassification`] (Data2VecAudio model)\n",
      "    - **hubert** -- [`HubertForSequenceClassification`] (Hubert model)\n",
      "    - **sew** -- [`SEWForSequenceClassification`] (SEW model)\n",
      "    - **sew-d** -- [`SEWDForSequenceClassification`] (SEW-D model)\n",
      "    - **unispeech** -- [`UniSpeechForSequenceClassification`] (UniSpeech model)\n",
      "    - **unispeech-sat** -- [`UniSpeechSatForSequenceClassification`] (UniSpeechSat model)\n",
      "    - **wav2vec2** -- [`Wav2Vec2ForSequenceClassification`] (Wav2Vec2 model)\n",
      "    - **wav2vec2-conformer** -- [`Wav2Vec2ConformerForSequenceClassification`] (Wav2Vec2-Conformer model)\n",
      "    - **wavlm** -- [`WavLMForSequenceClassification`] (WavLM model)\n",
      "    - **whisper** -- [`WhisperForAudioClassification`] (Whisper model)\n",
      "\n",
      "The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
      "deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
      "\n",
      "Args:\n",
      "    pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      "        Can be either:\n",
      "\n",
      "            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      "            - A path to a *directory* containing model weights saved using\n",
      "              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
      "            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
      "              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
      "              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
      "              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      "    model_args (additional positional arguments, *optional*):\n",
      "        Will be passed along to the underlying model `__init__()` method.\n",
      "    config ([`PretrainedConfig`], *optional*):\n",
      "        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      "        be automatically loaded when:\n",
      "\n",
      "            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
      "              model).\n",
      "            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
      "              save directory.\n",
      "            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
      "              configuration JSON file named *config.json* is found in the directory.\n",
      "    state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
      "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      "\n",
      "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      "        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
      "        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
      "    cache_dir (`str` or `os.PathLike`, *optional*):\n",
      "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      "        standard cache should not be used.\n",
      "    from_tf (`bool`, *optional*, defaults to `False`):\n",
      "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      "        `pretrained_model_name_or_path` argument).\n",
      "    force_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      "        cached versions if they exist.\n",
      "    resume_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      "        file exists.\n",
      "    proxies (`Dict[str, str]`, *optional*):\n",
      "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "    output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      "    local_files_only(`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to only look at local files (e.g., not try downloading the model).\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "        identifier allowed by git.\n",
      "    trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      "        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "        execute code present on the Hub on your local machine.\n",
      "    kwargs (additional keyword arguments, *optional*):\n",
      "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      "        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
      "        automatically loaded:\n",
      "\n",
      "            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
      "              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
      "              already been done)\n",
      "            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
      "              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
      "              corresponds to a configuration attribute will be used to override said attribute with the\n",
      "              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
      "              will be passed to the underlying model's `__init__` function.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoConfig, AutoModelForAudioClassification\n",
      "\n",
      ">>> # Download model and configuration from huggingface.co and cache.\n",
      ">>> model = AutoModelForAudioClassification.from_pretrained(\"bert-base-cased\")\n",
      "\n",
      ">>> # Update configuration during loading\n",
      ">>> model = AutoModelForAudioClassification.from_pretrained(\"bert-base-cased\", output_attentions=True)\n",
      ">>> model.config.output_attentions\n",
      "True\n",
      "\n",
      ">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
      ">>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
      ">>> model = AutoModelForAudioClassification.from_pretrained(\n",
      "...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
      "... )\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "AutoModelForAudioClassification.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81abbb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.weight_proj.weight', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da98c1c47f884f3aac8ba14c5b8726e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b54ca1a2cfd4c21ab13a9ced0b35e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f3d1e40836405f983da784f0b91847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2507a37b99e043ac8bb487b90996aae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8235766c584b8e919a6b60eb16ab2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb12260f5b4414685ca376c12a465a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in base_models:\n",
    "    pipeline(model=model, task='audio-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a89205-7452-4dd5-a55c-4a610297d43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
